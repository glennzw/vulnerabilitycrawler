   [PATCH] Fix zeroing on exception in copy_*_user
   
   - Don't zero for __copy_from_user_inatomic following i386.
   This will prevent spurious zeros for parallel file system writers when
   one does a exception
   - The string instruction version didn't zero the output on
   exception. Oops.
   
   Also I cleaned up the code a bit while I was at it and added a minor
   optimization to the string instruction path.
   
   Signed-off-by: Andi Kleen <ak@suse.de>
EXPORT_SYMBOL(copy_user_generic);
EXPORT_SYMBOL(copy_from_user);
EXPORT_SYMBOL(copy_to_user);

EXPORT_SYMBOL(copy_page);
EXPORT_SYMBOL(clear_page);

#define FIX_ALIGNMENT 1

	#include <asm/current.h>
	#include <asm/asm-offsets.h>
	#include <asm/thread_info.h>
	#include <asm/cpufeature.h>

/* Standard copy_to_user with segment limit checking */		
ENTRY(copy_to_user)
	jc  bad_to_user
	cmpq threadinfo_addr_limit(%rax),%rcx
	jae bad_to_user
2:
	.byte 0xe9	/* 32bit jump */
	.long .Lcug-1f
1:
	CFI_ENDPROC
ENDPROC(copy_to_user)

	.section .altinstr_replacement,"ax"
3:	.byte 0xe9			/* replacement jmp with 32 bit immediate */
	.long copy_user_generic_c-1b	/* offset */
	.previous
	.section .altinstructions,"a"
	.align 8
	.quad  2b
	.quad  3b
	.byte  X86_FEATURE_REP_GOOD
	.byte  5
	.byte  5
	.previous

/* Standard copy_from_user with segment limit checking */	
ENTRY(copy_from_user)
	jc  bad_from_user
	cmpq threadinfo_addr_limit(%rax),%rcx
	jae  bad_from_user
	/* FALL THROUGH to copy_user_generic */
	CFI_ENDPROC
ENDPROC(copy_from_user)
	
	
		
/*
 * copy_user_generic - memory copy with exception handling.
 * 	
 * Input:	
 * rdi destination
 * rsi source
 * rdx count
 *
 * Output:		
 * eax uncopied bytes or 0 if successful.
 */
ENTRY(copy_user_generic)
	CFI_STARTPROC
	.byte 0x66,0x66,0x90	/* 5 byte nop for replacement jump */
	.byte 0x66,0x90
1:
	.section .altinstr_replacement,"ax"
2:	.byte 0xe9	             /* near jump with 32bit immediate */
	.long copy_user_generic_c-1b /* offset */
	.previous
	.section .altinstructions,"a"
	.align 8
	.quad  copy_user_generic
	.quad  2b
	.byte  X86_FEATURE_REP_GOOD
	.byte  5
	.byte  5
	.previous
.Lcug:
	pushq %rbx
	CFI_ADJUST_CFA_OFFSET 8
	CFI_REL_OFFSET rbx, 0
	xorl %eax,%eax		/*zero for the exception handler */

#ifdef FIX_ALIGNMENT

	CFI_REMEMBER_STATE
.Lende:
	popq %rbx
	CFI_ADJUST_CFA_OFFSET -8
	CFI_RESTORE rbx
	addl %ecx,%edx
	/* edx: bytes to zero, rdi: dest, eax:zero */
.Lzero_rest:
	movq %rdx,%rcx
.Le_byte:
	xorl %eax,%eax
 /* rdi	destination
  * rsi source
  * rdx count
  *
  * Output:
  * eax uncopied bytes or 0 if successfull.
  * And more would be dangerous because both Intel and AMD have
  * errata with rep movsq > 4GB. If someone feels the need to fix
  * this please consider this.
   */
copy_user_generic_c:
	CFI_STARTPROC
	movl %edx,%ecx
	shrl $3,%ecx
	andl $7,%edx	
1:	rep 
	movsq 
	movl %edx,%ecx
2:	rep
	movsb
4:	movl %ecx,%eax
	ret
3:	lea (%rdx,%rcx,8),%rax
	ret
	CFI_ENDPROC
END(copy_user_generic_c)

	.section __ex_table,"a"
	.quad 1b,3b
	.quad 2b,4b
	.previous

/* Handles exceptions in both to and from, but doesn't do access_ok */
extern unsigned long copy_user_generic(void *to, const void *from, unsigned len); 

extern unsigned long copy_to_user(void __user *to, const void *from, unsigned len); 
extern unsigned long copy_from_user(void *to, const void __user *from, unsigned len); 
	}
}	


static __always_inline int __copy_in_user(void __user *dst, const void __user *src, unsigned size)
{ 
       int ret = 0;
unsigned long clear_user(void __user *mem, unsigned long len);
unsigned long __clear_user(void __user *mem, unsigned long len);

#define __copy_to_user_inatomic __copy_to_user
#define __copy_from_user_inatomic __copy_from_user

#endif /* __X86_64_UACCESS_H */
