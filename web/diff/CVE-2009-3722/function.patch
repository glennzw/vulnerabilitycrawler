commit 0a79b009525b160081d75cef5dbf45817956acf2
Author: Avi Kivity <avi@redhat.com>
Date:   Tue Sep 1 12:03:25 2009 +0300

    KVM: VMX: Check cpl before emulating debug register access
    
    Debug registers may only be accessed from cpl 0.  Unfortunately, vmx will
    code to emulate the instruction even though it was issued from guest
    userspace, possibly leading to an unexpected trap later.
    
    Cc: stable@kernel.org
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e8f166a..3be0004 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -620,6 +620,7 @@ void kvm_queue_exception(struct kvm_vcpu *vcpu, unsigned nr);
 void kvm_queue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code);
 void kvm_inject_page_fault(struct kvm_vcpu *vcpu, unsigned long cr2,
 			   u32 error_code);
+bool kvm_require_cpl(struct kvm_vcpu *vcpu, int required_cpl);
 
 int kvm_pic_set_irq(void *opaque, int irq, int level);
 
diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c
index cc6e00a..f381201 100644
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -2931,86 +2931,88 @@ static int handle_cr(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 static int handle_dr(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 {
 	unsigned long exit_qualification;
 	unsigned long val;
 	int dr, reg;
 
+	if (!kvm_require_cpl(vcpu, 0))
+		return 1;
 	dr = vmcs_readl(GUEST_DR7);
 	if (dr & DR7_GD) {
 		/*
 		 * As the vm-exit takes precedence over the debug trap, we
 		 * need to emulate the latter, either for the host or the
 		 * guest debugging itself.
 		 */
 		if (vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP) {
 			kvm_run->debug.arch.dr6 = vcpu->arch.dr6;
 			kvm_run->debug.arch.dr7 = dr;
 			kvm_run->debug.arch.pc =
 				vmcs_readl(GUEST_CS_BASE) +
 				vmcs_readl(GUEST_RIP);
 			kvm_run->debug.arch.exception = DB_VECTOR;
 			kvm_run->exit_reason = KVM_EXIT_DEBUG;
 			return 0;
 		} else {
 			vcpu->arch.dr7 &= ~DR7_GD;
 			vcpu->arch.dr6 |= DR6_BD;
 			vmcs_writel(GUEST_DR7, vcpu->arch.dr7);
 			kvm_queue_exception(vcpu, DB_VECTOR);
 			return 1;
 		}
 	}
 
 	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 	dr = exit_qualification & DEBUG_REG_ACCESS_NUM;
 	reg = DEBUG_REG_ACCESS_REG(exit_qualification);
 	if (exit_qualification & TYPE_MOV_FROM_DR) {
 		switch (dr) {
 		case 0 ... 3:
 			val = vcpu->arch.db[dr];
 			break;
 		case 6:
 			val = vcpu->arch.dr6;
 			break;
 		case 7:
 			val = vcpu->arch.dr7;
 			break;
 		default:
 			val = 0;
 		}
 		kvm_register_write(vcpu, reg, val);
 	} else {
 		val = vcpu->arch.regs[reg];
 		switch (dr) {
 		case 0 ... 3:
 			vcpu->arch.db[dr] = val;
 			if (!(vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP))
 				vcpu->arch.eff_db[dr] = val;
 			break;
 		case 4 ... 5:
 			if (vcpu->arch.cr4 & X86_CR4_DE)
 				kvm_queue_exception(vcpu, UD_VECTOR);
 			break;
 		case 6:
 			if (val & 0xffffffff00000000ULL) {
 				kvm_queue_exception(vcpu, GP_VECTOR);
 				break;
 			}
 			vcpu->arch.dr6 = (val & DR6_VOLATILE) | DR6_FIXED_1;
 			break;
 		case 7:
 			if (val & 0xffffffff00000000ULL) {
 				kvm_queue_exception(vcpu, GP_VECTOR);
 				break;
 			}
 			vcpu->arch.dr7 = (val & DR7_VOLATILE) | DR7_FIXED_1;
 			if (!(vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP)) {
 				vmcs_writel(GUEST_DR7, vcpu->arch.dr7);
 				vcpu->arch.switch_db_regs =
 					(val & DR7_BP_EN_MASK);
 			}
 			break;
 		}
 	}
 	skip_emulated_instruction(vcpu);
 	return 1;
 }
 
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 7627ff6..4137cc5 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -223,6 +223,19 @@ void kvm_queue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code)
 EXPORT_SYMBOL_GPL(kvm_queue_exception_e);
 
 /*
+ * Checks if cpl <= required_cpl; if true, return true.  Otherwise queue
+ * a #GP and return false.
+ */
+bool kvm_require_cpl(struct kvm_vcpu *vcpu, int required_cpl)
+{
+	if (kvm_x86_ops->get_cpl(vcpu) <= required_cpl)
+		return true;
+	kvm_queue_exception_e(vcpu, GP_VECTOR, 0);
+	return false;
+}
+EXPORT_SYMBOL_GPL(kvm_require_cpl);
+
+/*
  * Load the pae pdptrs.  Return true is they are all valid.
  */
 int load_pdptrs(struct kvm_vcpu *vcpu, unsigned long cr3)
