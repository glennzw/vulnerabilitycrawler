   net: sk_add_backlog() take rmem_alloc into account
   
   Current socket backlog limit is not enough to really stop DDOS attacks,
   because user thread spend many time to process a full backlog each
   round, and user might crazy spin on socket lock.
   
   We should add backlog size and receive_queue size (aka rmem_alloc) to
   pace writers, and let user run without being slow down too much.
   
   Introduce a sk_rcvqueues_full() helper, to avoid taking socket lock in
   stress situations.
   
   Under huge stress from a multiqueue/RPS enabled NIC, a single flow udp
   receiver can now process ~200.000 pps (instead of ~100 pps before the
   patch) on a 8 core machine.
   
   Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
   Signed-off-by: David S. Miller <davem@davemloft.net>
		struct sk_buff *head;
		struct sk_buff *tail;
		int len;
		int limit;
	} sk_backlog;
	wait_queue_head_t	*sk_sleep;
	struct dst_entry	*sk_dst_cache;
	skb->next = NULL;
}

/* The per-socket spinlock must be held here. */
static inline __must_check int sk_add_backlog(struct sock *sk, struct sk_buff *skb)
{
	if (sk->sk_backlog.len >= max(sk->sk_backlog.limit, sk->sk_rcvbuf << 1))
		return -ENOBUFS;

	__sk_add_backlog(sk, skb);

	skb->dev = NULL;

	if (nested)
		bh_lock_sock_nested(sk);
	else
	sk->sk_allocation	=	GFP_KERNEL;
	sk->sk_rcvbuf		=	sysctl_rmem_default;
	sk->sk_sndbuf		=	sysctl_wmem_default;
	sk->sk_backlog.limit	=	sk->sk_rcvbuf << 1;
	sk->sk_state		=	TCP_CLOSE;
	sk_set_socket(sk, sock);

			goto drop;
	}

	rc = 0;

	bh_lock_sock(sk);

		sk = stack[i];
		if (skb1) {
			bh_lock_sock(sk);
			if (!sock_owned_by_user(sk))
				udpv6_queue_rcv_skb(sk, skb1);

	/* deliver */

	bh_lock_sock(sk);
	if (!sock_owned_by_user(sk))
		udpv6_queue_rcv_skb(sk, skb);
	SCTP_DBG_OBJCNT_INC(sock);
	percpu_counter_inc(&sctp_sockets_allocated);

	/* Set socket backlog limit. */
	sk->sk_backlog.limit = sysctl_sctp_rmem[1];

	local_bh_disable();
	sock_prot_inuse_add(sock_net(sk), sk->sk_prot, 1);
	local_bh_enable();
