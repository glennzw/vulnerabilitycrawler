commit cb4644cac4a2797afc847e6c92736664d4b0ea34
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Wed Nov 10 14:36:25 2010 +0100

    bio: take care not overflow page count when mapping/copying user data
    
    If the iovec is being set up in a way that causes uaddr + PAGE_SIZE
    to overflow, we could end up attempting to map a huge number of
    pages. Check for this invalid input type.
    
    Reported-by: Dan Rosenberg <drosenberg@vsecurity.com>
    Cc: stable@kernel.org
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/fs/bio.c b/fs/bio.c
index 8317a2c..4bd454f 100644
--- a/fs/bio.c
+++ b/fs/bio.c
@@ -814,96 +814,102 @@ EXPORT_SYMBOL(bio_uncopy_user);
 struct bio *bio_copy_user_iov(struct request_queue *q,
 			      struct rq_map_data *map_data,
 			      struct sg_iovec *iov, int iov_count,
 			      int write_to_vm, gfp_t gfp_mask)
 {
 	struct bio_map_data *bmd;
 	struct bio_vec *bvec;
 	struct page *page;
 	struct bio *bio;
 	int i, ret;
 	int nr_pages = 0;
 	unsigned int len = 0;
 	unsigned int offset = map_data ? map_data->offset & ~PAGE_MASK : 0;
 
 	for (i = 0; i < iov_count; i++) {
 		unsigned long uaddr;
 		unsigned long end;
 		unsigned long start;
 
 		uaddr = (unsigned long)iov[i].iov_base;
 		end = (uaddr + iov[i].iov_len + PAGE_SIZE - 1) >> PAGE_SHIFT;
 		start = uaddr >> PAGE_SHIFT;
 
+		/*
+		 * Overflow, abort
+		 */
+		if (end < start)
+			return ERR_PTR(-EINVAL);
+
 		nr_pages += end - start;
 		len += iov[i].iov_len;
 	}
 
 	if (offset)
 		nr_pages++;
 
 	bmd = bio_alloc_map_data(nr_pages, iov_count, gfp_mask);
 	if (!bmd)
 		return ERR_PTR(-ENOMEM);
 
 	ret = -ENOMEM;
 	bio = bio_kmalloc(gfp_mask, nr_pages);
 	if (!bio)
 		goto out_bmd;
 
 	if (!write_to_vm)
 		bio->bi_rw |= REQ_WRITE;
 
 	ret = 0;
 
 	if (map_data) {
 		nr_pages = 1 << map_data->page_order;
 		i = map_data->offset / PAGE_SIZE;
 	}
 	while (len) {
 		unsigned int bytes = PAGE_SIZE;
 
 		bytes -= offset;
 
 		if (bytes > len)
 			bytes = len;
 
 		if (map_data) {
 			if (i == map_data->nr_entries * nr_pages) {
 				ret = -ENOMEM;
 				break;
 			}
 
 			page = map_data->pages[i / nr_pages];
 			page += (i % nr_pages);
 
 			i++;
 		} else {
 			page = alloc_page(q->bounce_gfp | gfp_mask);
 			if (!page) {
 				ret = -ENOMEM;
 				break;
 			}
 		}
 
 		if (bio_add_pc_page(q, bio, page, bytes, offset) < bytes)
 			break;
 
 		len -= bytes;
 		offset = 0;
 	}
 
 	if (ret)
 		goto cleanup;
 
 	/*
 	 * success
 	 */
 	if ((!write_to_vm && (!map_data || !map_data->null_mapped)) ||
 	    (map_data && map_data->from_user)) {
 		ret = __bio_copy_iov(bio, bio->bi_io_vec, iov, iov_count, 0, 1, 0);
 		if (ret)
 			goto cleanup;
 	}
 
 	bio_set_map_data(bmd, bio, iov, iov_count, map_data ? 0 : 1);
 	return bio;
@@ -947,118 +953,124 @@ EXPORT_SYMBOL(bio_copy_user);
 static struct bio *__bio_map_user_iov(struct request_queue *q,
 				      struct block_device *bdev,
 				      struct sg_iovec *iov, int iov_count,
 				      int write_to_vm, gfp_t gfp_mask)
 {
 	int i, j;
 	int nr_pages = 0;
 	struct page **pages;
 	struct bio *bio;
 	int cur_page = 0;
 	int ret, offset;
 
 	for (i = 0; i < iov_count; i++) {
 		unsigned long uaddr = (unsigned long)iov[i].iov_base;
 		unsigned long len = iov[i].iov_len;
 		unsigned long end = (uaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;
 		unsigned long start = uaddr >> PAGE_SHIFT;
 
+		/*
+		 * Overflow, abort
+		 */
+		if (end < start)
+			return ERR_PTR(-EINVAL);
+
 		nr_pages += end - start;
 		/*
 		 * buffer must be aligned to at least hardsector size for now
 		 */
 		if (uaddr & queue_dma_alignment(q))
 			return ERR_PTR(-EINVAL);
 	}
 
 	if (!nr_pages)
 		return ERR_PTR(-EINVAL);
 
 	bio = bio_kmalloc(gfp_mask, nr_pages);
 	if (!bio)
 		return ERR_PTR(-ENOMEM);
 
 	ret = -ENOMEM;
 	pages = kcalloc(nr_pages, sizeof(struct page *), gfp_mask);
 	if (!pages)
 		goto out;
 
 	for (i = 0; i < iov_count; i++) {
 		unsigned long uaddr = (unsigned long)iov[i].iov_base;
 		unsigned long len = iov[i].iov_len;
 		unsigned long end = (uaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;
 		unsigned long start = uaddr >> PAGE_SHIFT;
 		const int local_nr_pages = end - start;
 		const int page_limit = cur_page + local_nr_pages;
-		
+
 		ret = get_user_pages_fast(uaddr, local_nr_pages,
 				write_to_vm, &pages[cur_page]);
 		if (ret < local_nr_pages) {
 			ret = -EFAULT;
 			goto out_unmap;
 		}
 
 		offset = uaddr & ~PAGE_MASK;
 		for (j = cur_page; j < page_limit; j++) {
 			unsigned int bytes = PAGE_SIZE - offset;
 
 			if (len <= 0)
 				break;
 			
 			if (bytes > len)
 				bytes = len;
 
 			/*
 			 * sorry...
 			 */
 			if (bio_add_pc_page(q, bio, pages[j], bytes, offset) <
 					    bytes)
 				break;
 
 			len -= bytes;
 			offset = 0;
 		}
 
 		cur_page = j;
 		/*
 		 * release the pages we didn't map into the bio, if any
 		 */
 		while (j < page_limit)
 			page_cache_release(pages[j++]);
 	}
 
 	kfree(pages);
 
 	/*
 	 * set data direction, and check if mapped pages need bouncing
 	 */
 	if (!write_to_vm)
 		bio->bi_rw |= REQ_WRITE;
 
 	bio->bi_bdev = bdev;
 	bio->bi_flags |= (1 << BIO_USER_MAPPED);
 	return bio;
 
  out_unmap:
 	for (i = 0; i < nr_pages; i++) {
 		if(!pages[i])
 			break;
 		page_cache_release(pages[i]);
 	}
  out:
 	kfree(pages);
 	bio_put(bio);
 	return ERR_PTR(ret);
 }
 
 /**
  *	bio_map_user	-	map user address into bio
  *	@q: the struct request_queue for the bio
  *	@bdev: destination block device
  *	@uaddr: start of user address
  *	@len: length in bytes
  *	@write_to_vm: bool indicating writing to pages or not
  *	@gfp_mask: memory allocation flags
  *
  *	Map the user space address into a bio suitable for io to a block
  *	device. Returns an error pointer in case of error.
  */
